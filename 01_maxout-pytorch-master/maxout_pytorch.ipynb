{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maxout Networks in Pytorch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is an implementation of maxout units (https://arxiv.org/abs/1302.4389) in Pytorch. My primary motivation of this was to ensure that I actually understood what maxout units were, as I found most explanations available quite opaque or incorrect! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "from torchvision import datasets, transforms \n",
    "from torch.autograd import Variable \n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we define a custom object so we can have models which contain objects which behave like lists while holding multiple modules. This is needed as we're going to store lists of layers per maxout unit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ListModule(object):\n",
    "    def __init__(self, module, prefix, *args):\n",
    "        self.module = module\n",
    "        self.prefix = prefix\n",
    "        self.num_module = 0\n",
    "        for new_module in args:\n",
    "            self.append(new_module)\n",
    "\n",
    "    def append(self, new_module):\n",
    "        if not isinstance(new_module, nn.Module):\n",
    "            raise ValueError('Not a Module')\n",
    "        else:\n",
    "            self.module.add_module(self.prefix + str(self.num_module), new_module)\n",
    "            self.num_module += 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_module\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        if i < 0 or i >= self.num_module:\n",
    "            raise IndexError('Out of bound')\n",
    "        return getattr(self.module, self.prefix + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "batch_size = 50\n",
    "n_epochs = 3\n",
    "cuda = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The magic normalization numbers below are actually the mean and the std of the training set of MNIST, hardcoded to save compute each run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c10e51bed66c4fc0a3743359c3a70c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce802e8bfaa42429e405ff21bd1a377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e707fa1cf4bd47d88d167c35d8fefdcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad45667c59e24bb090c018a8480850e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "                       batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=False, download=True, \n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])),\n",
    "                       batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Maxout? \n",
    "Maxout is a piece of a deep learning model that sits somewhere between an activation function and a layer. It computes a nonlinearity (namely the max function) but also has learnable parameters. For reference, it is generally considered an activation but I find that terminology troublesome as it forces there to be multiple layers to max over. `¯\\_(ツ)_/¯`\n",
    "\n",
    "In their paper, Goodfellow et. al. describe the maxout activation as: \n",
    "\n",
    "$$h_i = max_{j \\in [1,k]} z_{i,j}$$\n",
    "\n",
    "where $h_i$ is the hidden layer output and $z = x^TW_{\\cdots i,j} + b_{i,j}$ , where $x$ could be the input or the output of the previous layer. \n",
    "\n",
    "\n",
    "I find this description incredibly confusing and for a while I thought that all that you did for maxout was to compute a linear layer, group into k groups, and take the max over those groups. (the use of $k$ in the indices above should give you a hint that this interpretation is wrong) Alas, Karpathy was to the rescue here: http://cs231n.github.io/neural-networks-1/\n",
    "\n",
    "The tldr; is that a maxout unit is an element-wise max _over_ multiple linear (or convolutional/other) layers, so for a maxout layer with two units it is computed, like so: \n",
    "$max\\{W_1^Tx +b_1, W_2^Tx + b_2\\}$\n",
    "\n",
    "so in the case where $W_2^Tx+b_2 = 0$ we see that the maxout layer is a ReLU. For other properties about maxout nets, we refer back to Goodfellow's paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Models \n",
    "Here we define the two classes of models to test the maxout network against. \n",
    "\n",
    "We have an MLP and ConvNet using just ReLU's and the maxout equivalent of those networks. \n",
    "\n",
    "Note that for each layer which uses a maxout we have $lu$ total parameters, where $l$ is the number of parameters per unit and $u$ is the number of units. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class rectifier_mlp(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(rectifier_mlp, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class maxout_mlp(nn.Module):\n",
    "    def __init__(self, num_units=2):\n",
    "        super(maxout_mlp, self).__init__()\n",
    "        self.fc1_list = ListModule(self, \"fc1_\")\n",
    "        self.fc2_list = ListModule(self, \"fc2_\")\n",
    "        for _ in range(num_units):\n",
    "            self.fc1_list.append(nn.Linear(784, 1024))\n",
    "            self.fc2_list.append(nn.Linear(1024, 10))\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = x.view(-1, 784)\n",
    "        x = self.maxout(x, self.fc1_list)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.maxout(x, self.fc2_list)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "    def maxout(self, x, layer_list):\n",
    "        max_output = layer_list[0](x)\n",
    "        for _, layer in enumerate(layer_list, start=1):\n",
    "            max_output = torch.max(max_output, layer(x))\n",
    "        return max_output\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class rectifier_conv_net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(rectifier_conv_net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5, padding=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5, padding=2)\n",
    "        self.fc1 = nn.Linear(64*7*7, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), 2)\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, 64*7*7)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class maxout_conv_net(nn.Module):\n",
    "    def __init__(self, num_units=2):\n",
    "        super(maxout_conv_net, self).__init__()\n",
    "        self.conv1_list = ListModule(self, \"conv1_\")\n",
    "        self.conv2_list = ListModule(self, \"conv2_\")\n",
    "        self.fc1_list = ListModule(self, \"fc1_\")\n",
    "        self.fc2_list = ListModule(self, \"fc2_\")\n",
    "        for _ in range(num_units):\n",
    "            self.conv1_list.append(nn.Conv2d(1, 32, 5, padding=2))\n",
    "            self.conv2_list.append(nn.Conv2d(32, 64, 5, padding=2))\n",
    "            self.fc1_list.append(nn.Linear(64*7*7, 1024))\n",
    "            self.fc2_list.append(nn.Linear(1024, 10))\n",
    "\n",
    "    def forward(self, x): \n",
    "        x = F.max_pool2d(self.maxout(x, self.conv1_list), 2)\n",
    "        x = F.max_pool2d(self.maxout(x, self.conv2_list), 2)\n",
    "        x = x.view(-1, 64*7*7)\n",
    "        x = self.maxout(x, self.fc1_list)\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.maxout(x, self.fc2_list)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "    def maxout(self, x, layer_list):\n",
    "        max_output = layer_list[0](x)\n",
    "        for _, layer in enumerate(layer_list, start=1):\n",
    "            max_output = torch.max(max_output, layer(x))\n",
    "        return max_output\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(epoch,  net, train_loss, train_acc): \n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    net.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader): \n",
    "        if cuda:\n",
    "            data, target = Variable(data).cuda(0), Variable(target).cuda(0)\n",
    "        else:\n",
    "            data, target = Variable(data), Variable(target)\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        output = net(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        train_loss.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        prediction = output.data.max(1)[1]\n",
    "        accuracy = prediction.eq(target.data).sum()*1.0/batch_size*100.0 \n",
    "        train_acc.append(accuracy)\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test(epoch, net):\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        if cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data, target = Variable(data, volatile=True), Variable(target)\n",
    "        output = net(data)\n",
    "        test_loss += F.cross_entropy(output, target).item()\n",
    "        pred = output.data.max(1)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(target.data).cpu().sum()\n",
    "\n",
    "    test_loss = test_loss\n",
    "    test_loss /= len(test_loader) # loss function already averages over batch size\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yasho\\AppData\\Local\\Temp\\ipykernel_116\\3476879234.py:17: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.303015\n",
      "Train Epoch: 0 [5000/60000 (8%)]\tLoss: 0.419752\n",
      "Train Epoch: 0 [10000/60000 (17%)]\tLoss: 0.266622\n",
      "Train Epoch: 0 [15000/60000 (25%)]\tLoss: 0.145577\n",
      "Train Epoch: 0 [20000/60000 (33%)]\tLoss: 0.262069\n",
      "Train Epoch: 0 [25000/60000 (42%)]\tLoss: 0.123064\n",
      "Train Epoch: 0 [30000/60000 (50%)]\tLoss: 0.229677\n",
      "Train Epoch: 0 [35000/60000 (58%)]\tLoss: 0.118933\n",
      "Train Epoch: 0 [40000/60000 (67%)]\tLoss: 0.102190\n",
      "Train Epoch: 0 [45000/60000 (75%)]\tLoss: 0.067328\n",
      "Train Epoch: 0 [50000/60000 (83%)]\tLoss: 0.023033\n",
      "Train Epoch: 0 [55000/60000 (92%)]\tLoss: 0.042585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yasho\\AppData\\Local\\Temp\\ipykernel_116\\1272325452.py:8: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0564, Accuracy: 9829/10000 (98%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.126205\n",
      "Train Epoch: 1 [5000/60000 (8%)]\tLoss: 0.151101\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 0.046178\n",
      "Train Epoch: 1 [15000/60000 (25%)]\tLoss: 0.065658\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 0.025831\n",
      "Train Epoch: 1 [25000/60000 (42%)]\tLoss: 0.033142\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 0.017649\n",
      "Train Epoch: 1 [35000/60000 (58%)]\tLoss: 0.132626\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.011232\n",
      "Train Epoch: 1 [45000/60000 (75%)]\tLoss: 0.023515\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 0.019395\n",
      "Train Epoch: 1 [55000/60000 (92%)]\tLoss: 0.030575\n",
      "\n",
      "Test set: Average loss: 0.0382, Accuracy: 9876/10000 (99%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.039112\n",
      "Train Epoch: 2 [5000/60000 (8%)]\tLoss: 0.057213\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 0.010218\n",
      "Train Epoch: 2 [15000/60000 (25%)]\tLoss: 0.016414\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.059671\n",
      "Train Epoch: 2 [25000/60000 (42%)]\tLoss: 0.051451\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 0.011266\n",
      "Train Epoch: 2 [35000/60000 (58%)]\tLoss: 0.007559\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.004825\n",
      "Train Epoch: 2 [45000/60000 (75%)]\tLoss: 0.014703\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 0.040268\n",
      "Train Epoch: 2 [55000/60000 (92%)]\tLoss: 0.009622\n",
      "\n",
      "Test set: Average loss: 0.0321, Accuracy: 9899/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if cuda:\n",
    "    net_relu = rectifier_conv_net().cuda()\n",
    "else:\n",
    "    net_relu = rectifier_conv_net()\n",
    "relu_loss = [] \n",
    "relu_acc = []\n",
    "for epoch in range(n_epochs): \n",
    "    train(epoch, net_relu, relu_loss, relu_acc)\n",
    "    test(epoch, net_relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yasho\\AppData\\Local\\Temp\\ipykernel_116\\3933911939.py:21: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.360898\n",
      "Train Epoch: 0 [5000/60000 (8%)]\tLoss: 0.307796\n",
      "Train Epoch: 0 [10000/60000 (17%)]\tLoss: 0.211857\n",
      "Train Epoch: 0 [15000/60000 (25%)]\tLoss: 0.097797\n",
      "Train Epoch: 0 [20000/60000 (33%)]\tLoss: 0.144156\n",
      "Train Epoch: 0 [25000/60000 (42%)]\tLoss: 0.104841\n",
      "Train Epoch: 0 [30000/60000 (50%)]\tLoss: 0.085498\n",
      "Train Epoch: 0 [35000/60000 (58%)]\tLoss: 0.174965\n",
      "Train Epoch: 0 [40000/60000 (67%)]\tLoss: 0.041148\n",
      "Train Epoch: 0 [45000/60000 (75%)]\tLoss: 0.222196\n",
      "Train Epoch: 0 [50000/60000 (83%)]\tLoss: 0.058747\n",
      "Train Epoch: 0 [55000/60000 (92%)]\tLoss: 0.025577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yasho\\AppData\\Local\\Temp\\ipykernel_116\\1272325452.py:8: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0493, Accuracy: 9837/10000 (98%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.027061\n",
      "Train Epoch: 1 [5000/60000 (8%)]\tLoss: 0.070990\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 0.024793\n",
      "Train Epoch: 1 [15000/60000 (25%)]\tLoss: 0.127609\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 0.028530\n",
      "Train Epoch: 1 [25000/60000 (42%)]\tLoss: 0.011766\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 0.010946\n",
      "Train Epoch: 1 [35000/60000 (58%)]\tLoss: 0.036566\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.008922\n",
      "Train Epoch: 1 [45000/60000 (75%)]\tLoss: 0.072018\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 0.112786\n",
      "Train Epoch: 1 [55000/60000 (92%)]\tLoss: 0.006450\n",
      "\n",
      "Test set: Average loss: 0.0425, Accuracy: 9869/10000 (99%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.028124\n",
      "Train Epoch: 2 [5000/60000 (8%)]\tLoss: 0.028138\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 0.121977\n",
      "Train Epoch: 2 [15000/60000 (25%)]\tLoss: 0.007587\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.052519\n",
      "Train Epoch: 2 [25000/60000 (42%)]\tLoss: 0.046518\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 0.017308\n",
      "Train Epoch: 2 [35000/60000 (58%)]\tLoss: 0.003024\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.012271\n",
      "Train Epoch: 2 [45000/60000 (75%)]\tLoss: 0.032371\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 0.005622\n",
      "Train Epoch: 2 [55000/60000 (92%)]\tLoss: 0.086498\n",
      "\n",
      "Test set: Average loss: 0.0273, Accuracy: 9912/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if cuda:\n",
    "    net_maxout = maxout_conv_net(num_units=5).cuda() #this uses 5 \"maxout units\" per \"layer\" \n",
    "else:\n",
    "    net_maxout = maxout_conv_net(num_units=5)\n",
    "maxout_loss = [] \n",
    "maxout_acc = []\n",
    "for epoch in range(n_epochs): \n",
    "    train(epoch, net_maxout, maxout_loss, maxout_acc)\n",
    "    test(epoch, net_maxout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.348691\n",
      "Train Epoch: 0 [5000/60000 (8%)]\tLoss: 0.682529\n",
      "Train Epoch: 0 [10000/60000 (17%)]\tLoss: 0.646795\n",
      "Train Epoch: 0 [15000/60000 (25%)]\tLoss: 0.302137\n",
      "Train Epoch: 0 [20000/60000 (33%)]\tLoss: 0.265129\n",
      "Train Epoch: 0 [25000/60000 (42%)]\tLoss: 0.374851\n",
      "Train Epoch: 0 [30000/60000 (50%)]\tLoss: 0.476626\n",
      "Train Epoch: 0 [35000/60000 (58%)]\tLoss: 0.351321\n",
      "Train Epoch: 0 [40000/60000 (67%)]\tLoss: 0.422480\n",
      "Train Epoch: 0 [45000/60000 (75%)]\tLoss: 0.324298\n",
      "Train Epoch: 0 [50000/60000 (83%)]\tLoss: 0.330992\n",
      "Train Epoch: 0 [55000/60000 (92%)]\tLoss: 0.216023\n",
      "\n",
      "Test set: Average loss: 0.2178, Accuracy: 9364/10000 (94%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.397623\n",
      "Train Epoch: 1 [5000/60000 (8%)]\tLoss: 0.346569\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 0.124273\n",
      "Train Epoch: 1 [15000/60000 (25%)]\tLoss: 0.188086\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 0.086380\n",
      "Train Epoch: 1 [25000/60000 (42%)]\tLoss: 0.271180\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 0.160728\n",
      "Train Epoch: 1 [35000/60000 (58%)]\tLoss: 0.398956\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.136644\n",
      "Train Epoch: 1 [45000/60000 (75%)]\tLoss: 0.100698\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 0.144666\n",
      "Train Epoch: 1 [55000/60000 (92%)]\tLoss: 0.105870\n",
      "\n",
      "Test set: Average loss: 0.1531, Accuracy: 9554/10000 (96%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.146667\n",
      "Train Epoch: 2 [5000/60000 (8%)]\tLoss: 0.111268\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 0.150802\n",
      "Train Epoch: 2 [15000/60000 (25%)]\tLoss: 0.102013\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.167289\n",
      "Train Epoch: 2 [25000/60000 (42%)]\tLoss: 0.114355\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 0.123657\n",
      "Train Epoch: 2 [35000/60000 (58%)]\tLoss: 0.154903\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.087987\n",
      "Train Epoch: 2 [45000/60000 (75%)]\tLoss: 0.081518\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 0.083032\n",
      "Train Epoch: 2 [55000/60000 (92%)]\tLoss: 0.119544\n",
      "\n",
      "Test set: Average loss: 0.1170, Accuracy: 9666/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if cuda:\n",
    "    mlp_relu = rectifier_mlp().cuda() \n",
    "else:\n",
    "    mlp_relu = rectifier_mlp()\n",
    "relu_mlp_loss = [] \n",
    "relu_mlp_acc = []\n",
    "for epoch in range(n_epochs): \n",
    "    train(epoch, mlp_relu, relu_mlp_loss, relu_mlp_acc)\n",
    "    test(epoch, mlp_relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.452453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yasho\\AppData\\Local\\Temp\\ipykernel_116\\4226546613.py:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [5000/60000 (8%)]\tLoss: 0.459581\n",
      "Train Epoch: 0 [10000/60000 (17%)]\tLoss: 0.370488\n",
      "Train Epoch: 0 [15000/60000 (25%)]\tLoss: 0.156213\n",
      "Train Epoch: 0 [20000/60000 (33%)]\tLoss: 0.199699\n",
      "Train Epoch: 0 [25000/60000 (42%)]\tLoss: 0.149305\n",
      "Train Epoch: 0 [30000/60000 (50%)]\tLoss: 0.135788\n",
      "Train Epoch: 0 [35000/60000 (58%)]\tLoss: 0.302234\n",
      "Train Epoch: 0 [40000/60000 (67%)]\tLoss: 0.089232\n",
      "Train Epoch: 0 [45000/60000 (75%)]\tLoss: 0.233280\n",
      "Train Epoch: 0 [50000/60000 (83%)]\tLoss: 0.160067\n",
      "Train Epoch: 0 [55000/60000 (92%)]\tLoss: 0.128705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yasho\\AppData\\Local\\Temp\\ipykernel_116\\1272325452.py:8: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  data, target = Variable(data, volatile=True), Variable(target)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.1384, Accuracy: 9600/10000 (96%)\n",
      "\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 0.174342\n",
      "Train Epoch: 1 [5000/60000 (8%)]\tLoss: 0.178904\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 0.155739\n",
      "Train Epoch: 1 [15000/60000 (25%)]\tLoss: 0.118325\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 0.082263\n",
      "Train Epoch: 1 [25000/60000 (42%)]\tLoss: 0.117883\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 0.197321\n",
      "Train Epoch: 1 [35000/60000 (58%)]\tLoss: 0.059323\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.127476\n",
      "Train Epoch: 1 [45000/60000 (75%)]\tLoss: 0.134717\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 0.253097\n",
      "Train Epoch: 1 [55000/60000 (92%)]\tLoss: 0.157242\n",
      "\n",
      "Test set: Average loss: 0.0932, Accuracy: 9731/10000 (97%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.027247\n",
      "Train Epoch: 2 [5000/60000 (8%)]\tLoss: 0.192539\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 0.042914\n",
      "Train Epoch: 2 [15000/60000 (25%)]\tLoss: 0.062732\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.074999\n",
      "Train Epoch: 2 [25000/60000 (42%)]\tLoss: 0.068081\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 0.052132\n",
      "Train Epoch: 2 [35000/60000 (58%)]\tLoss: 0.090164\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.084168\n",
      "Train Epoch: 2 [45000/60000 (75%)]\tLoss: 0.147892\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 0.060343\n",
      "Train Epoch: 2 [55000/60000 (92%)]\tLoss: 0.046231\n",
      "\n",
      "Test set: Average loss: 0.0732, Accuracy: 9778/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if cuda:\n",
    "    mlp_maxout = maxout_mlp(num_units=5).cuda() \n",
    "else:\n",
    "    mlp_maxout = maxout_mlp(num_units=5)\n",
    "maxout_mlp_loss = [] \n",
    "maxout_mlp_acc = []\n",
    "for epoch in range(n_epochs): \n",
    "    train(epoch, mlp_maxout, maxout_mlp_loss, maxout_mlp_acc)\n",
    "    test(epoch, mlp_maxout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots! \n",
    "Here I show the accuracy and loss of both the MLP and the ConvNet versions of the ReLU and maxout nets. \n",
    "\n",
    "These plots aren't incredibly informative as MNIST is so easy that both nets do very well quickly. That said, it is clear from the plots and numbers above that the maxout network is doing better than the ReLU net. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'relu_mlp_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_116\\1722911184.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"paper\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mplot1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaxout_mlp_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'maxout_mlp'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mplot2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrelu_mlp_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu_mlp'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mplot1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'relu_mlp_loss' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_context(\"paper\")\n",
    "plot1, = plt.plot(maxout_mlp_loss, label='maxout_mlp')\n",
    "plot2, = plt.plot(relu_mlp_loss, label='relu_mlp')\n",
    "plt.legend(handles=[plot1, plot2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_116\\3233672002.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplot1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaxout_mlp_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxout_mlp_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'maxout_conv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplot2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrelu_mlp_acc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrelu_mlp_acc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu_conv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mplot1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplot2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2767\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2768\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2769\u001b[1;33m     return gca().plot(\n\u001b[0m\u001b[0;32m   2770\u001b[0m         \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2771\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1630\u001b[0m         \"\"\"\n\u001b[0;32m   1631\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1632\u001b[1;33m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1633\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1634\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    310\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[1;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[0;32m    486\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    487\u001b[0m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 488\u001b[1;33m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    489\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m             \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindex_of\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxy\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\matplotlib\\cbook\\__init__.py\u001b[0m in \u001b[0;36m_check_1d\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_unpack_to_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1305\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'shape'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1307\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36matleast_1d\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\numpy\\core\\shape_base.py\u001b[0m in \u001b[0;36matleast_1d\u001b[1;34m(*arys)\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mary\u001b[0m \u001b[1;32min\u001b[0m \u001b[0marys\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mary\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    755\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__array__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 757\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    758\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    759\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot1, = plt.plot(np.arange(len(maxout_mlp_acc)), maxout_mlp_acc, label='maxout_conv')\n",
    "plot2, = plt.plot(np.arange(len(relu_mlp_acc)), relu_mlp_acc, label='relu_conv')\n",
    "plt.legend(handles=[plot1, plot2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion \n",
    "\n",
    "Maxout units are cool! I'm interested to see how much of an improvement these make on lower dimensional linear layers at the end of large convolutional nets and will be testing that in the coming weeks. The lack of saturation might help gradients back prop, while not requiring that you have many times more parameters for you entire model. \n",
    "\n",
    "This work also made me realize viscerally how important clear explanations are! There are *so many* incorrect implementations that you can find online of maxout units, so hopefully this will serve as a guide for people who were in my position a couple of days ago. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References \n",
    "- https://arxiv.org/pdf/1302.4389.pdf (original paper) \n",
    "- http://cs231n.github.io/neural-networks-1/ (Karpathy to the rescue)\n",
    "- http://www.simon-hohberg.de/2015/07/19/maxout.html (really helpful visualizations) \n",
    "- tons of Pytorch forum posts, thanks y'all "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
